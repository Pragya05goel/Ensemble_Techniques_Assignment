{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "325b1498-3ff3-4eb1-83d3-b9acfc456d54",
   "metadata": {},
   "source": [
    "# **ASSIGNMENT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137f347-f4d3-41b3-85c0-720276e128c5",
   "metadata": {},
   "source": [
    "**Q1. What is an ensemble technique in machine learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08708f64-0e8b-4cb9-9519-43e3e70dd6a0",
   "metadata": {},
   "source": [
    "An ensemble technique in machine learning involves combining predictions from multiple models to create a more robust and accurate predictive model than any individual model on its own. The idea is that by leveraging the strengths of different models and compensating for their weaknesses, the ensemble can achieve better performance and generalization on diverse datasets.\n",
    "\n",
    "There are several types of ensemble techniques, but two main categories are:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating):** In bagging, multiple instances of the same base learning algorithm are trained on different subsets of the training data. These subsets are usually created by random sampling with replacement (bootstrap sampling). After training, predictions from each model are combined through averaging (for regression problems) or voting (for classification problems).\n",
    "\n",
    "   - **Example:** Random Forest is a popular ensemble method that employs bagging. It builds multiple decision trees and combines their predictions.\n",
    "\n",
    "2. **Boosting:** In boosting, base models are trained sequentially, and each subsequent model focuses on correcting the errors made by the previous ones. Instances that are misclassified by earlier models are given more weight, so subsequent models pay more attention to them.\n",
    "\n",
    "   - **Example:** AdaBoost (Adaptive Boosting) is a well-known boosting algorithm that combines weak learners to create a strong learner.\n",
    "\n",
    "Ensemble methods are widely used in machine learning because they often lead to improved performance, robustness, and generalization. Popular ensemble algorithms include Random Forest, Gradient Boosting Machines (GBM), XGBoost, LightGBM, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e78e5-8b05-40ec-bcab-a07391cf366c",
   "metadata": {},
   "source": [
    "**Q2. Why are ensemble techniques used in machine learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c81e4-cc57-4688-9d01-c689cb99d165",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several reasons, and they offer various advantages that contribute to improved model performance. Here are some key reasons why ensemble techniques are widely employed:\n",
    "\n",
    "1. **Improved Accuracy and Generalization:**\n",
    "   - Ensembles often achieve higher accuracy than individual models. By combining the predictions of multiple models, the strengths of some models can compensate for the weaknesses of others. This leads to more robust and accurate predictions, particularly in situations where individual models may struggle.\n",
    "\n",
    "2. **Reduction of Overfitting:**\n",
    "   - Ensemble methods can help reduce overfitting, especially in complex models. Overfitting occurs when a model learns the training data too well, capturing noise and outliers that don't generalize to new, unseen data. Ensembles, particularly bagging methods, can mitigate overfitting by averaging or voting over multiple models, which helps smooth out individual model idiosyncrasies.\n",
    "\n",
    "3. **Handling Noisy Data and Outliers:**\n",
    "   - Ensembles are robust in the presence of noisy data and outliers. Outliers or mislabeled instances may have a disproportionate impact on a single model, but their influence is often mitigated when combined with predictions from other models in an ensemble.\n",
    "\n",
    "4. **Increased Robustness:**\n",
    "   - Ensembles are more robust in the face of changes in the input data. Since they rely on the collective behavior of multiple models, they are less sensitive to small variations in the training data and are less likely to be influenced by outliers or anomalies.\n",
    "\n",
    "5. **Versatility Across Algorithms:**\n",
    "   - Ensemble methods are versatile and can be applied to different base learning algorithms. This flexibility allows practitioners to combine the strengths of various algorithms and adapt to the specific characteristics of the data.\n",
    "\n",
    "6. **Easy Parallelization:**\n",
    "   - Some ensemble methods, particularly bagging techniques, are inherently parallelizable. This makes them well-suited for parallel computing environments, enabling faster model training and prediction.\n",
    "\n",
    "7. **Applicability to Various Problem Types:**\n",
    "   - Ensemble techniques can be applied to a wide range of machine learning problems, including classification, regression, and even unsupervised learning tasks. Different ensemble algorithms can be tailored to specific problem types.\n",
    "\n",
    "Overall, ensemble techniques are a valuable tool in the machine learning toolbox, providing a practical means to enhance model performance, robustness, and generalization across diverse applications. Popular ensemble algorithms like Random Forest, Gradient Boosting Machines (GBM), and others have been successfully applied in many real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ae0a8-71c7-42ab-8dcb-56af75d34ff6",
   "metadata": {},
   "source": [
    "**Q3. What is bagging?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dc8352-7b27-43f5-a8cb-b9c6cc97adf0",
   "metadata": {},
   "source": [
    "Bagging, which stands for Bootstrap Aggregating, is an ensemble technique in machine learning that involves training multiple instances of the same learning algorithm on different subsets of the training data. The primary idea behind bagging is to introduce diversity among the models by training them on various subsets of the data, thus reducing overfitting and improving the overall performance and robustness of the ensemble.\n",
    "\n",
    "Here are the key steps involved in the bagging process:\n",
    "\n",
    "1. **Bootstrap Sampling:**\n",
    "   - Random subsets of the training data are created by sampling with replacement. This means that some instances may be included multiple times in a subset, while others may be left out.\n",
    "\n",
    "2. **Model Training:**\n",
    "   - A base learning algorithm (e.g., decision tree, neural network, etc.) is trained independently on each of these bootstrap samples. Each instance of the algorithm is trained on a slightly different variation of the training data.\n",
    "\n",
    "3. **Prediction Aggregation:**\n",
    "   - After training, predictions are made by each model on the entire dataset (including the instances not included in their respective bootstrap samples). For regression problems, the predictions are typically averaged, while for classification problems, a majority voting scheme is often used.\n",
    "\n",
    "4. **Final Prediction:**\n",
    "   - The final prediction for a new instance is determined by aggregating the predictions from all the individual models.\n",
    "\n",
    "**Advantages of Bagging:**\n",
    "- **Reduction of Overfitting:** Bagging helps reduce overfitting by training models on diverse subsets of the data and then combining their predictions.\n",
    "  \n",
    "- **Increased Stability:** It improves the stability and robustness of the model by reducing the variance associated with individual models.\n",
    "\n",
    "- **Parallelization:** The training of each model can be done independently, making bagging methods highly parallelizable and efficient.\n",
    "\n",
    "- **Applicability to Various Algorithms:** Bagging can be applied to various base learning algorithms, such as decision trees, neural networks, and others.\n",
    "\n",
    "**Example: Random Forest:**\n",
    "One of the most popular bagging algorithms is Random Forest. In Random Forest, multiple decision trees are trained on different bootstrap samples, and their predictions are combined through a voting mechanism (for classification) or averaging (for regression). Random Forest is known for its versatility and robust performance across different types of datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7480b00-57ae-4d0f-b19e-717620e39486",
   "metadata": {},
   "source": [
    "**Q4. What is boosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eca4ec-7a13-4ed3-8e8c-3baf7b92d2f8",
   "metadata": {},
   "source": [
    "Boosting is another ensemble technique in machine learning that aims to improve the accuracy of a model by combining the predictions of weak learners, typically decision trees. Unlike bagging, where models are trained independently, boosting builds a sequence of models, and each subsequent model focuses on correcting the errors made by the previous ones. The key idea behind boosting is to give more weight to instances that are misclassified by earlier models, forcing subsequent models to pay more attention to these instances.\n",
    "\n",
    "Here are the main steps involved in the boosting process:\n",
    "\n",
    "1. **Weak Learner Training:**\n",
    "   - A weak learner (often a shallow decision tree) is trained on the original dataset. It performs slightly better than random chance but is not necessarily a strong model on its own.\n",
    "\n",
    "2. **Instance Weighting:**\n",
    "   - Instances that are misclassified by the weak learner are given higher weights. This means that the next weak learner will focus more on the instances that the previous models found challenging.\n",
    "\n",
    "3. **Model Combination:**\n",
    "   - The predictions from each weak learner are combined with a weighted sum, giving more influence to models that perform well on the misclassified instances.\n",
    "\n",
    "4. **Update Weights:**\n",
    "   - The weights of the instances are updated based on the performance of the ensemble so far. Misclassified instances receive higher weights, and correctly classified instances receive lower weights.\n",
    "\n",
    "5. **Iterative Process:**\n",
    "   - Steps 1-4 are repeated for a predefined number of iterations or until a certain level of performance is achieved. Each new weak learner is trained to correct the errors of the ensemble up to that point.\n",
    "\n",
    "6. **Final Prediction:**\n",
    "   - The final prediction is made by combining the predictions of all the weak learners, typically through a weighted sum.\n",
    "\n",
    "**Advantages of Boosting:**\n",
    "- **Improved Accuracy:** Boosting often leads to models with higher accuracy compared to individual weak learners.\n",
    "  \n",
    "- **Effective Handling of Complex Relationships:** Boosting is particularly effective in capturing complex relationships in the data, making it suitable for a wide range of tasks.\n",
    "\n",
    "- **Reduced Bias:** Boosting helps reduce bias by iteratively focusing on instances that are challenging for the current ensemble.\n",
    "\n",
    "- **Versatility Across Algorithms:** Boosting can be applied to different base learning algorithms, although decision trees are commonly used.\n",
    "\n",
    "**Example: AdaBoost (Adaptive Boosting):**\n",
    "AdaBoost is a well-known boosting algorithm. It assigns different weights to instances and adjusts these weights at each iteration to emphasize the misclassified instances. Weak learners are combined through a weighted sum to form a strong learner. AdaBoost has been widely used for both binary and multiclass classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2a03b-3883-4fb4-90e4-818e67cba7de",
   "metadata": {},
   "source": [
    "**Q5. What are the benefits of using ensemble techniques?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54612177-50b1-4fb5-8439-4e78ae00c951",
   "metadata": {},
   "source": [
    "Ensemble techniques offer several benefits in machine learning, making them popular and widely used in various applications. Here are some key advantages of using ensemble techniques:\n",
    "\n",
    "1. **Improved Accuracy:**\n",
    "   - Ensemble methods often lead to higher accuracy compared to individual models. By combining the predictions of multiple models, ensemble techniques can mitigate errors and improve overall predictive performance.\n",
    "\n",
    "2. **Enhanced Generalization:**\n",
    "   - Ensembles are better at generalizing to new, unseen data. The diversity among the models helps reduce overfitting and makes the ensemble more robust across different subsets of the data.\n",
    "\n",
    "3. **Increased Robustness:**\n",
    "   - Ensembles are more robust in the face of noisy data and outliers. Outliers or mislabeled instances are less likely to have a significant impact on the overall ensemble predictions.\n",
    "\n",
    "4. **Reduced Variance:**\n",
    "   - Ensemble methods, especially bagging techniques, help reduce variance by averaging or combining predictions from multiple models. This can lead to more stable and reliable models.\n",
    "\n",
    "5. **Handling of Model Biases:**\n",
    "   - Ensemble methods can mitigate biases associated with individual models. By combining models with different strengths and weaknesses, ensemble techniques provide a more balanced and unbiased prediction.\n",
    "\n",
    "6. **Versatility Across Algorithms:**\n",
    "   - Ensemble techniques are versatile and can be applied to a wide range of base learning algorithms. This flexibility allows practitioners to leverage the strengths of different algorithms and adapt to the specific characteristics of the data.\n",
    "\n",
    "7. **Effective in High-Dimensional Spaces:**\n",
    "   - In high-dimensional feature spaces, where individual models may struggle, ensembles can capture complex relationships and interactions more effectively, leading to better performance.\n",
    "\n",
    "8. **Parallelization and Efficiency:**\n",
    "   - Some ensemble methods, particularly bagging techniques, are inherently parallelizable. This makes them well-suited for parallel computing environments, enabling faster model training and prediction.\n",
    "\n",
    "9. **Ease of Implementation:**\n",
    "   - Implementing ensemble techniques is often straightforward, especially with popular libraries that provide pre-built ensemble algorithms. This ease of use makes ensembles accessible to a wide range of practitioners.\n",
    "\n",
    "10. **Adaptability to Various Problem Types:**\n",
    "    - Ensemble techniques can be applied to various types of machine learning problems, including classification, regression, and unsupervised learning. Different ensemble algorithms can be tailored to specific problem domains.\n",
    "\n",
    "11. **Ensemble Diversity:**\n",
    "    - The strength of ensemble methods lies in the diversity of individual models. By combining models that make different types of errors, ensemble techniques can achieve better overall performance.\n",
    "\n",
    "Overall, the benefits of ensemble techniques make them a valuable tool for improving the performance, robustness, and generalization of machine learning models across different domains and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd8e5ef-4ebb-4d0c-a042-711196e2a950",
   "metadata": {},
   "source": [
    "**Q6. Are ensemble techniques always better than individual models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad426479-0644-41e4-b505-7cbe8da22f1c",
   "metadata": {},
   "source": [
    "While ensemble techniques can offer significant improvements in many cases, they are not guaranteed to be better than individual models in every scenario. The effectiveness of ensemble methods depends on various factors, and there are situations where using an ensemble may not provide substantial benefits. Here are some considerations:\n",
    "\n",
    "1. **Data Size and Quality:**\n",
    "   - In scenarios where the dataset is small or of low quality, ensemble techniques may not always outperform individual models. Ensembles often excel when there is sufficient diverse data to train multiple models effectively.\n",
    "\n",
    "2. **Computational Cost:**\n",
    "   - Ensemble methods, especially boosting algorithms, can be computationally expensive and may require more resources compared to training a single model. In situations where computational resources are limited, using a simpler model might be preferred.\n",
    "\n",
    "3. **Simple and Well-Performing Models:**\n",
    "   - If the individual models in consideration are already strong performers on their own, the marginal gain from creating an ensemble may be minimal. In such cases, the additional complexity introduced by an ensemble may not be justified.\n",
    "\n",
    "4. **Overfitting Concerns:**\n",
    "   - While ensemble methods, particularly bagging, can help reduce overfitting, there are cases where overfitting might still occur, especially if the base models are complex. It's important to monitor and manage overfitting in ensemble methods.\n",
    "\n",
    "5. **Interpretability:**\n",
    "   - Individual models are often more interpretable than ensemble models, especially when using complex algorithms like gradient boosting. If interpretability is a critical requirement, a single, interpretable model might be preferred.\n",
    "\n",
    "6. **Algorithm Suitability:**\n",
    "   - The choice of the base learning algorithm matters. Some algorithms may not benefit as much from ensembling, and there are cases where a well-tuned individual model may perform comparably or even better than an ensemble.\n",
    "\n",
    "7. **Time Constraints:**\n",
    "   - In time-sensitive applications, building and training multiple models as part of an ensemble may not be practical. In such cases, a quicker-to-train individual model might be preferred.\n",
    "\n",
    "8. **Noise in the Data:**\n",
    "   - If the dataset contains a significant amount of noise or irrelevant features, ensembling may not always lead to better performance. The diversity in models might not effectively address noise, and simpler models or feature engineering might be more beneficial.\n",
    "\n",
    "In summary, while ensemble techniques are powerful tools in the machine learning toolbox, their effectiveness depends on the specific characteristics of the data and the problem at hand. It's essential to consider the trade-offs, computational costs, and other factors when deciding whether to use ensemble techniques or rely on individual models. Experimentation and empirical evaluation on a specific dataset are often necessary to determine the most suitable approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16cf44-c533-4c64-965d-d684fa307632",
   "metadata": {},
   "source": [
    "**Q7. How is the confidence interval calculated using bootstrap?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472e1d8-7f82-48e4-bf3c-858e0b07da43",
   "metadata": {},
   "source": [
    "Bootstrapping is a resampling technique that involves repeatedly sampling with replacement from the observed data to estimate the sampling distribution of a statistic. One common application of bootstrapping is to calculate confidence intervals. Here's a general outline of how the confidence interval is calculated using the bootstrap method:\n",
    "\n",
    "1. **Collect Bootstrap Samples:**\n",
    "   - Randomly draw multiple bootstrap samples (with replacement) from the observed data. Each bootstrap sample has the same size as the original dataset.\n",
    "\n",
    "2. **Compute Statistic:**\n",
    "   - For each bootstrap sample, calculate the statistic of interest. This could be the mean, median, standard deviation, or any other relevant statistic.\n",
    "\n",
    "3. **Create Bootstrap Distribution:**\n",
    "   - Collect all the computed statistics from the bootstrap samples to form the bootstrap distribution of the statistic.\n",
    "\n",
    "4. **Calculate Confidence Interval:**\n",
    "   - Determine the confidence interval from the bootstrap distribution. The confidence interval is typically defined by the percentiles of the bootstrap distribution.\n",
    "\n",
    "   - For example, a 95% confidence interval would be obtained by finding the 2.5th percentile and the 97.5th percentile of the bootstrap distribution.\n",
    "\n",
    "   - If \\(B\\) is the number of bootstrap samples, and \\(1-\\alpha\\) is the desired confidence level, the confidence interval is often calculated from percentiles \\(\\alpha/2\\) and \\(1-\\alpha/2\\) of the bootstrap distribution.\n",
    "\n",
    "   - The formula for the confidence interval is: \\((\\text{{Percentile}}_{\\alpha/2}, \\text{{Percentile}}_{1-\\alpha/2})\\)\n",
    "\n",
    "Here's a more detailed step-by-step breakdown:\n",
    "\n",
    "1. **Collect Bootstrap Samples:**\n",
    "   - Let \\(B\\) be the number of bootstrap samples.\n",
    "   - For \\(i = 1\\) to \\(B\\):\n",
    "     - Randomly draw a sample with replacement from the observed data.\n",
    "\n",
    "2. **Compute Statistic:**\n",
    "   - For each bootstrap sample, calculate the statistic of interest (e.g., mean, median, standard deviation).\n",
    "\n",
    "3. **Create Bootstrap Distribution:**\n",
    "   - Form a distribution of the calculated statistics from the bootstrap samples.\n",
    "\n",
    "4. **Calculate Confidence Interval:**\n",
    "   - Determine the \\(\\alpha/2\\) and \\(1-\\alpha/2\\) percentiles of the bootstrap distribution.\n",
    "   - The interval between these percentiles forms the bootstrap confidence interval.\n",
    "\n",
    "In summary, bootstrapping provides a way to estimate the uncertainty associated with a statistic by resampling from the observed data. The resulting confidence interval gives a range of plausible values for the parameter of interest based on the variability observed in the bootstrap samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a3212a-6948-444d-9b69-57dcc540052f",
   "metadata": {},
   "source": [
    "**Q8. How does bootstrap work and What are the steps involved in bootstrap?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f57790-577a-4606-918d-ce11a419ec5f",
   "metadata": {},
   "source": [
    "Bootstrap is a statistical resampling technique used to estimate the sampling distribution of a statistic by repeatedly resampling with replacement from the observed data. It allows for making inferences about the population distribution without assuming a specific parametric form. The basic idea is to mimic the process of drawing samples from the population by repeatedly sampling from the observed data.\n",
    "\n",
    "Here are the general steps involved in the bootstrap procedure:\n",
    "\n",
    "1. **Original Data:**\n",
    "   - Start with a dataset of size \\(n\\) containing observed data points.\n",
    "\n",
    "2. **Resampling with Replacement:**\n",
    "   - Randomly draw \\(n\\) samples with replacement from the observed data. This means that each draw is independent, and the same data point can be selected multiple times in a single bootstrap sample.\n",
    "\n",
    "3. **Sample Statistic:**\n",
    "   - Calculate the statistic of interest (e.g., mean, median, standard deviation) on the newly created bootstrap sample.\n",
    "\n",
    "4. **Repeat Steps 2-3:**\n",
    "   - Repeat steps 2 and 3 a large number of times (e.g., thousands of times) to generate a collection of bootstrap samples and their associated statistics.\n",
    "\n",
    "5. **Bootstrap Distribution:**\n",
    "   - Collect the calculated statistics from each bootstrap sample to create the bootstrap distribution of the statistic.\n",
    "\n",
    "6. **Statistical Inference:**\n",
    "   - Use the bootstrap distribution to make statistical inferences about the population parameter or to estimate the uncertainty associated with the statistic of interest.\n",
    "\n",
    "The key concept behind bootstrap is that the distribution of the sample statistic calculated from the bootstrap samples approximates the sampling distribution of the statistic in the population. This approach is particularly useful when analytical methods for deriving the distribution of the statistic are difficult or impossible.\n",
    "\n",
    "Bootstrap can be applied to various statistical problems, including estimating confidence intervals, standard errors, and bias, as well as constructing hypothesis tests. It provides a flexible and computationally straightforward way to assess the variability and uncertainty associated with sample statistics.\n",
    "\n",
    "In summary, the bootstrap method involves resampling with replacement from the observed data to create a distribution of the statistic of interest, allowing for statistical inference without making strong parametric assumptions about the underlying population distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ba434a-f6b1-4e52-8c54-a1c44364372b",
   "metadata": {},
   "source": [
    "**Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d647cf-1221-406a-b954-fe37ceb3e031",
   "metadata": {},
   "source": [
    "Certainly! To estimate the 95% confidence interval for the population mean height using bootstrap, you can follow these steps:\n",
    "\n",
    "1. **Collect the Original Sample:**\n",
    "   - Start with the original sample of 50 tree heights, including the mean (\\(\\bar{x}\\)) and standard deviation (\\(s\\)).\n",
    "\n",
    "   \\(\\bar{x}_{\\text{original}} = 15\\) meters  \n",
    "   \\(s_{\\text{original}} = 2\\) meters\n",
    "\n",
    "2. **Bootstrap Resampling:**\n",
    "   - Generate a large number of bootstrap samples by randomly sampling with replacement from the original sample.\n",
    "\n",
    "3. **Calculate Bootstrap Sample Mean:**\n",
    "   - For each bootstrap sample, calculate the sample mean (\\(\\bar{x}_{\\text{bootstrap}}\\)).\n",
    "\n",
    "4. **Repeat Steps 2-3:**\n",
    "   - Repeat steps 2 and 3, e.g., 10,000 times.\n",
    "\n",
    "5. **Create Bootstrap Distribution:**\n",
    "   - Collect all the bootstrap sample means to form the distribution of sample means.\n",
    "\n",
    "6. **Calculate Confidence Interval:**\n",
    "   - Determine the 2.5th and 97.5th percentiles of the bootstrap distribution to construct the 95% confidence interval.\n",
    "\n",
    "   \\[\\text{95% Confidence Interval} = (\\text{Percentile}_{2.5}, \\text{Percentile}_{97.5})\\]\n",
    "\n",
    "7. **Estimate the Confidence Interval:**\n",
    "   - Calculate the values for the percentiles based on the bootstrap distribution.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f45b2082-ab04-4757-a787-5eb6c89a1520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval: [14.23921658 15.4639785 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original sample\n",
    "original_sample = np.random.normal(loc=15, scale=2, size=50)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_bootstrap_samples = 10000\n",
    "\n",
    "# Bootstrap resampling\n",
    "bootstrap_means = np.zeros(num_bootstrap_samples)\n",
    "\n",
    "for i in range(num_bootstrap_samples):\n",
    "    # Generate a bootstrap sample\n",
    "    bootstrap_sample = np.random.choice(original_sample, size=len(original_sample), replace=True)\n",
    "    \n",
    "    # Calculate the mean of the bootstrap sample\n",
    "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(\"95% Confidence Interval:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14bc4a9-6979-4edd-843e-ee1e8439c16f",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d87a9e9-2cca-4f1f-abb9-c8556b1057b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
